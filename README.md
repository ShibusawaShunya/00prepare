【事前準備】
今コードでは、python標準のライブラリ以外で必要となるライブラリをインストールする。
コマンドは以下の四つである。

pip install numpy
pip install matplotlib
pip install sox
pip install pyyaml

実行環境のコマンドプロンプト、コンソールで上記のコマンドを実行する。またPytorch(https://pytorch.org)をインストールする。
Pytorchを使用するプログラムは、GPUおよびCUDAを使用できる環境で実行することを前提としている。


※soxを使用する際にエラーが出てしまう場合は、Homebrewをインストールしてsoxをインストールする。
Homebrewとは、macOSオペレーティングシステム上のパッケージ管理システムのひとつである。

具体的な手順は以下の項目である。
1,macOS上でコマンドプロンプトを開き、
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
上記のコードをコピペする。

2,インストール完了後以下のコードをコピペする。
”brew install sox”
以上で完了である。
  
  
【実行手順】
"00download_data.py"⇨"01prepare_wav.py"⇨"02prepare/label.py"⇨"03subset_data.py"⇨"plot_wave.py"⇨"test_fft.py"⇨"test_spectrogram.py"⇨
"01_compute_fbank.py"⇨"test_cepstrum.py"⇨"02_compute_mean_std.py"


【コード解説】
”00download_data.py”
JSUTコーパスとそのラベルデータ一式をダウンロードし、所定のフォルダに展開するソースコード。
このコードを実行すると、"00prepare"と同じ階層に”data”というフォルダが作成され、”data/original”にコーパスとラベルデータ一式が展開される。
展開された”data/original/jsut_ver1.1”には”basic5000”や”countersuffix26”などのフォルダがあり、各フォルダの中には音声ファイルが含まれている。今回は主に”basic5000”に含まれる音声データを使用する。


”01prepare_wav.py”
ダウンサンプリングをするソースコード。
このコードを実行すると、”data/original/jsut_ver1.1/basic5000”に含まれる音声データをサンプリングレート16kHzの音声に変換し、”data/wav”フォルダに保存する。
また、”data/wav”フォルダに保存した音声ファイルのリストを、”data/label/all/wav.scp”に保存する。また、"data/wav"フォルダに保存した、
音声ファイルのリストを、”data/label/all/wav.scp”に出力する。


”02prepare/label.py”
テキストラベルファイルを準備するソースコード。
発話ごとの”text_level2”、"kana_level3"、"phone_level3"の内容を抽出し、1文字（キャラクター）単位、平仮名(かな)単位、音素単位でそれぞれ定義した
3種類のテキストラベルを、”data/label/all”フォルダに、"text_char"、"text_kana"、"text_phone"というファイル名で出力する。この時、テキストから句読点は除去している。


”03subset_data.py”
作成した音声ファイルリストとテキストラベルリストを分割し、学習データ、開発データ、評価データのリストを作成するソースコード。
BASIC5000_0001〜BASIC5000_5000のうち、0001〜0250を評価データ、0251〜0500を開発データに使用する。残りは学習データに使用するが、
0501〜5000を全て用いる学習データ(大)と0501〜1500のみを用いる学習データ(小)の2種類のデータを作成する。

プログラみを実行すると、"data/label/all"に格納されていた音声ファイルリストとテキストラベルリストがそれぞれ分割され、学習データ(小)、学習データ(大)、開発データ、評価データが
それぞれ"data/label"以下の"train_small"、"train_large"、"dev"、"test"というフォルダに出力される。


"plot_wave.py"
音声波形を描画するソースコード。
pythonでwavファイルを開き、音圧値の総サンプル数とその値を読むことのできるモジュールであるwaveと、wavファイルを読み込み、グラフに描画するためのmatplotlibを用いる。

コードを実行すると、"data/wav/BASIC5000_0001.wav"を開き、サンプリング周波数、サンプルサイズ、チャネル数、音圧値の総サンプル数とその値を読み込み、各情報を出力するとともに、
波形のプロット図を"plot.png"というファイルに出力する。"with wave.open［wav ファイルパス］ as wav:"によってwavファイルを開き、
サンプリング周波数、サンプルサイズ、チャネル数、音圧値の総サンプル数をそれぞれ"wav.getframerate［］"、""wav.getsampwidth［］、"wav.getnchannels［］"、"wav.getnframes［］"によって取得する。
ただし、サンプルサイズはビットではなくバイトで得られるため、16ビット/8=2バイトとなる。総サンプル数は51,040である。サンプリング周波数、つまり1秒間に含まれるサンプル数は16,000個のため、
音声の時間長は51,040/16,000 = 3.19秒となる。
各サンプルの音圧値を読み込むには、"waveform = wav.readframes［読み込むサンプル数］"とする。読み込んだ時点でデータはバイナリ型となっているため、
数値演算ライブラリであるnumpyの"np.frombuffer［waveform, dtype=np.int16］"とすることで、バイナリ型から16ビット数値型に変換する。


”test_fft.py”
wavファイルを開き、特定時刻の時間波形に対してフーリエ変換し、振幅スペクトルを計算するソースコード。

このコードでは、"BASIC5000_0001.wav"(発話内容：「水をマレーシアから買わなくてはならないのです。」)を開き、「を」(音素は/o/)を発話している時刻0.58秒から1,024サンプル分を取り出す。
取り出した1,024サンプル分の時間波形"frame"に対して"np.fft.fft［frame］"とすることで、複素スペクトルを計算する。次に複素スペクトルの絶対値を取ることで振幅スペクトルを計算する。
振幅スペクトルは左右対称であり、分析には左側しか使わないため、absoluteの0〜512までを取り出す。


"test_spectrogram.py"
短時間のフーリエ変換を行うソースコード。
wavファイルを読み込んだあと、フレームサイズ"frame_size"とフレームシフト"frame_shift"をサンプル数に変換する。この時、フレームサイズ、つまり1フレームに含まれる波形データ数は
16,000［Hz］×25［ミリ秒］×0.001 = 400である。ただし、高速フーリエ変換をする際のサンプル数は2のべき乗である必要である。そのため、実際のサンプル数400より大きく、
かつ最も近い2のべき乗を"while fft_size < frame_size:fft_size =2"の部分で求める。この例ではfft_sizeは512になる。次に短時間フーリエ変換をしたときの総フレーム数を事前に計算する。
ここでは、分析開始点をフレームシフトの分だけ増やしていき、分析開始点＋フレームサイズが波形の終端を超えた場合は、その時点で分析を終了するようにしている。

フレーム番号"frame_idx"の分析開始点は、frame_idx × frame_shiftである。まず、分析開始点からフレームサイズ分だけ波形を取り出す。次に取り出したフレームデータに対して、ハミング窓をかける。
ハミング窓関数はnumpyライブラリの"np.hamming［frame_size］"で定義されている。そのあと、"np.fft.fft［］"で高速フーリエ変換を行うが引数に"n = fft_size"が入っている。
これによりデータ数400のフレームデータに対して、データ数が512になるまで後ろに0を加えたあと、高速フーリエ変換が行われる。
その後、振幅スペクトルを計算し、行数が総フレーム数、行列がfft_size/2+1の行列を作成する。⇦をスペクトログラムと呼ぶ。
スペクトログラムを作成後、描画領域を２分割し、上側に時間波形を、下側にスペクトログラムを表示させる。スペクトログラムの表示にはmatplotlibの"plt.imshow［］"を使っている。
ただし、スペクトログラムをそのまま表示させると縦軸が時間、横軸が周波数となってしまうため、転置したあとさらに上下を反転させている。


"01_compute_fbank.py"
対数メルフィルバンク特徴を計算するソースコード。
このコードでは、特徴量抽出を行うクラスとして、"class FeatureExtractor"を作成している。
このクラスでは、メル周波数ケプストラム(MFCC)を計算する機能を備えている。

メイン関数では、まず"feat_extractor = FeatureExtractor［］"として特徴量抽出クラスを呼び出している。
この時、フレームサイズは25ミリ秒、フレームシフトは10ミリ秒、メルフィルタバンクの次元数(フィルタの数)を40、ディザリングの係数を1.0としている。
また分析を行う周波数範囲［Flow,Fhigh］をそれぞれ20HzとFs/2にしている。範囲を20Hz以上にしているのは、人間が知覚できる音の周波数がおよそ20Hz以上であることと、低周波数帯域には収録機器の電気的なノイズが含まれることがあるためである。
"FeatureExtractor"を呼び出したとき、コンストラクト("__init__"で定義されている関数のこと)内で、関数"MakeMelFilterBank［］"が呼び出され、メルフィルタバンクが作成される。
wavファイルから読み込んだデータ"waveform"に対して、"feat_extractor.computeFBANK［waveform］"とすることで、メルフィルタバンク特徴を計算する。
"ComputeFBANK"内では短時間分析を行うが、waveformから1フレーム分のデータを抽出したあと、ディザリング、直流成分除去、高域強調、ハミング窓の適用をおこなっている。
このように処理されたフレームデータに対して、パワースペクトルを計算し、メルフィルタバンクを畳み込んだ後に、その対数をとったものを"fbank_features"として出力する。
このプログラムは実際の音声認識実験を想定して、学習データ(小、大)、開発データ、評価データそれぞれに対して対数メルフィルタバンク特徴を計算し、保存している。


"test_cepstrum.py"
ケプストラム分析を行い、対数パワースペクトルの包絡成分と微細胞構造成分に分解するソースコード。
"BASIC5000_0001.wav"(発話内容：「水をマレーシアから買わなくてはならないのです。」)の「を」(音素は/o/)を発話している時間帯に対してケプストラム分析をおこなっている。
対数パワースペクトルを計算した後、逆フーリエ変換を行うことでケプストラムを計算している。求めたケプストラムに対して、高ケフレンシ帯域をカットしたものと低ケフレンシ帯域をカットしたものを作成し
それぞれ再度フーリエ変換により対数パワースペクトルに戻し、プロットしている。
